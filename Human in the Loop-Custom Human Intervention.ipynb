{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ccbc2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Literal, Dict, Any \n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, END, MessagesState \n",
    "from langchain_groq import ChatGroq \n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from dotenv import load_dotenv \n",
    "from langchain_classic.agents import initialize_agent, AgentType\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.tools.arxiv import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper \n",
    "from langgraph.types import Interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "30bdab2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c80d6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model = \"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e635a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_input = input(\"Please enter your topic: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "08398de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is attention is all youu need in nlp?'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fec89b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisorState(MessagesState):\n",
    "    \"\"\"State for the multi-agent\"\"\"\n",
    "    next_agent: str = \"supervisor\"\n",
    "    web_data: str = \"\"\n",
    "    arxiv_data: str = \"\"\n",
    "    final_report: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c56023a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_agent(state: SupervisorState) -> Dict:\n",
    "    has_web_data = bool(state.get(\"web_data\"))\n",
    "    has_arxiv_data = bool(state.get(\"arxiv_data\"))\n",
    "    has_final_report = bool(state.get(\"final_report\"))\n",
    "\n",
    "    if not has_web_data and not has_arxiv_data and not has_final_report:\n",
    "        next_agent = \"web_search\"\n",
    "    \n",
    "    elif has_web_data and not has_arxiv_data and not has_final_report:\n",
    "        next_agent = \"writer\"\n",
    "\n",
    "    elif has_web_data and has_final_report:\n",
    "        user_input = input(prompt = \"Do you want a deeper answer? [y/n]\")\n",
    "        if user_input == \"y\":\n",
    "            next_agent = \"arxiv_search\"\n",
    "        else:\n",
    "            next_agent = \"__end__\"\n",
    "        \n",
    "    if has_web_data and has_arxiv_data and has_final_report:\n",
    "        next_agent = \"__end__\"\n",
    "\n",
    "    return {\n",
    "        \"next_agent\": next_agent\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aea1b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_agent(state: SupervisorState) -> Dict:\n",
    "    \"\"\"Gathers data from web\"\"\"\n",
    "    b = []\n",
    "    messages = state.get(\"messages\", [])\n",
    "    user_message = \"\"\n",
    "    for m in messages:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            user_message = m.content\n",
    "    tavily = TavilySearch()\n",
    "    response = tavily.invoke({\"query\": user_message})\n",
    "    for item in response.get(\"results\", []):\n",
    "        b.append(item.get(\"content\", \"\"))\n",
    "    \n",
    "    return {\n",
    "        \"web_data\": b,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f48ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_agent(state: SupervisorState) -> Dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    user_message = \"\"\n",
    "    for m in messages:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            user_message = m.content\n",
    "\n",
    "    wrapper = ArxivAPIWrapper(top_k_results=5)\n",
    "    arxiv = ArxivQueryRun(api_wrapper=wrapper)\n",
    "    result = arxiv.invoke(user_message)\n",
    "\n",
    "    return {\n",
    "        \"arxiv_data\": result,\n",
    "        \"stage\": \"after_arxiv\",\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0fc1d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer_agent(state: SupervisorState) -> Dict:\n",
    "    \"\"\"write a report\"\"\"\n",
    "    message = state.get(\"messages\")\n",
    "    for m in message:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            question = m.content\n",
    "\n",
    "    web_data = state.get(\"web_data\", \"\")\n",
    "    arxiv_data = state.get(\"arxiv_data\", \"\")\n",
    "\n",
    "    writing_prompt = f\"\"\"\n",
    "Answer the question based only on the text below:\n",
    "\n",
    "question: {question}\n",
    "\n",
    "text: {web_data} | {arxiv_data}\n",
    "\"\"\"\n",
    "\n",
    "    report_response = llm.invoke([HumanMessage(content = writing_prompt)])\n",
    "    report = report_response.content\n",
    "\n",
    "    final_report = f\"\"\"\n",
    "    Final Report: \n",
    "{'=' * 50}\n",
    "topic: {message}\n",
    "{'=' * 50}\n",
    "\n",
    "{report}\n",
    "\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"final_report\": final_report,\n",
    "        \"next_agent\": \"supervisor\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: SupervisorState):\n",
    "    next_agent = state.get(\"next_agent\", \"supervisor\")\n",
    "\n",
    "    if next_agent in [\"__end__\", \"end\"]:\n",
    "        return END\n",
    "    \n",
    "    if next_agent in [\"supervisor\", \"web_search\", \"writer\", \"arxiv_search\"]:\n",
    "        return next_agent \n",
    "    \n",
    "    return \"supervisor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "385d692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(SupervisorState)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"web_search\", web_agent)\n",
    "workflow.add_node(\"arxiv_search\", arxiv_agent)\n",
    "workflow.add_node(\"writer\", writer_agent)\n",
    "\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "for node in [\"supervisor\", \"web_search\", \"arxiv_search\", \"writer\"]:\n",
    "    workflow.add_conditional_edges(\n",
    "        node,\n",
    "        router,\n",
    "        {\n",
    "            \"supervisor\": \"supervisor\",\n",
    "            \"web_search\": \"web_search\",\n",
    "            \"arxiv_search\": \"arxiv_search\",\n",
    "            \"writer\": \"writer\",\n",
    "            \"__end__\": END\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fbe79a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "78b74db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content = query_input)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "796e5b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='what is attention is all youu need in nlp?', additional_kwargs={}, response_metadata={}, id='03543f1c-e923-4845-a397-1aa71878d786')], 'next_agent': '__end__', 'web_data': ['## The Paper that changed AI Forever. The 2017 paper *â€œAttention Is All You Needâ€* introduced the Transformer model, which has revolutionized Natural Language Processing (NLP) and machine learning as a whole. It replaced traditional models that relied heavily on recurrence and convolution. In this blog, I will break down the key ideas from the paper into digestible parts, offering a simplified summary while retaining a technical touch. Before this paper, most NLP models were based on Recurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM) networks. * **Limited parallelism**: RNNs process data sequentially, which makes them slow for long sequences. The authors of *â€œAttention Is All You Needâ€* proposed the **Transformer**, a model that relies entirely on the mechanism of **attention** without using any recurrence or convolution. This model efficiently handles long-range dependencies, can be trained faster, and processes input in parallel, making it groundbreaking for modern AI. Simplifying tech, Building the future ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ Tech Enthusiast with 10+ Years of experience in software design, AWS Cloud, AI and many more.', 'The paper introduced a new deep learning architecture known as the transformer \"Transformer (machine learning model)\"), based on the attention mechanism proposed in 2014 by Bahdanau *et al.* It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, *decomposable attention* applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs. One of its authors, Jakob Uszkoreit, suspected that attention *without* recurrence would be sufficient for language translation, thus the title \"attention is *all* you need\".', 'By focusing on attention mechanisms, the Transformer architecture overcomes the limitations of traditional RNN and CNN models, enabling more efficient and effective sequence processing. â€¢ Self-Attention Mechanism: This allows the model to weigh the importance of different words in the input sequence when processing each word, capturing contextual relationships. 1. Self-Attention Mechanism: This allows the model to weigh the importance of different words in the input sequence when processing each word, capturing contextual relationships. In summary, attention mechanisms in the Transformer allow the model to dynamically focus on different parts of the input sequence, capturing both local and long-range dependencies. As we continue to explore and expand upon the Transformer architecture, itâ€™s worth reflecting on the elegant simplicity of the core idea: that attention â€” the ability to dynamically focus on relevant parts of the input â€” is all you need to build powerful sequence processing models.', \"View a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors. The best performing models also connect the encoder and decoder through an attention mechanism. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. | Cite as: | arXiv:1706.03762 [cs.CL] |. |  | (or  arXiv:1706.03762v7 [cs.CL] for this version) |. **** Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB). **** Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB). **[v7]** Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB). View a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors. ### References & Citations. # Bibliographic and Citation Tools. Have an idea for a project that will add value for arXiv's community?\", '3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: â€¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. 7 Conclusion In this work, we presented the Transformer, the ï¬rst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.'], 'arxiv_data': 'Published: 2023-10-25\\nTitle: Topological Complexity Related To Multi-Valued Functions\\nAuthors: Melih Ä°s\\nSummary: In this paper, we deal with the robot motion planning problem in multi-valued function theory. We first enrich the multi-homotopy studies by introducing a multi-homotopy lifting property and a multi-fibration. Then we compute both a topological multi-complexity and a Lusternik-Schnirelmann multi-category of a space or a multi-fibration.\\n\\nPublished: 2022-03-04\\nTitle: Different Types of Topological Complexity on Higher Homotopic Distance\\nAuthors: Melih Ä°s, Ä°smet Karaca\\nSummary: We first study the higher version of the relative topological complexity by using the homotopic distance. We also introduced the generalized version of the relative topological complexity of a topological pair on both the Schwarz genus and the homotopic distance. With these concepts, we give some inequalities including the topological complexity and the Lusternik-Schnirelmann category, the most important parts of the study of robot motion planning in topology. Finally, by defining the parametrised topological complexity via the homotopic distance, we present some estimates on the higher setting of this concept.\\n\\nPublished: 2024-08-28\\nTitle: $m-$homotopic Distances in Digital Images\\nAuthors: Melih Ä°s, Ä°smet Karaca\\nSummary: We define digital $m-$homotopic distance and its higher version. We also mention related notions such as $m-$category in the sense of Lusternik-Schnirelmann and $m-$complexity in topological robotics. Later, we examine the homotopy invariance or $m-$homotopy invariance property of these concepts.\\n\\nPublished: 2020-09-01\\nTitle: Topological Complexities of Finite Digital Images\\nAuthors: Melih Ä°s, Ä°smet Karaca\\nSummary: Digital topological methods are often used on computing the topological complexity of digital images. We give new results on the relation between reducibility and digital contractibility in order to determine the topological complexity of a digitally connected finite digital image. We present all possible cases of the topological complexity TC of a finite digital image in Z and Z^2$. Finally, we determine the higher topological complexity TC_{n} of finite irreducible digital images independently of the number of points for n > 1.\\n\\nPublished: 2024-03-09\\nTitle: Discrete Topological Complexities of Simplical Maps\\nAuthors: Melih Ä°s, Ä°smet Karaca\\nSummary: In this study, we delve into the discrete TC of surjective simplicial fibrations, aiming to unravel the interplay between topological complexity, discrete geometric structures, and computational efficiency. Moreover, we examine the properties of the discrete TC number in higher dimensions and its relationship with scat. We also touch on the basic properties of the notion of higher contiguity distance, and show that it is possible to consider discrete TC computations in a simpler sense.', 'final_report': '\\n    Final Report: \\n==================================================\\ntopic: [HumanMessage(content=\\'what is attention is all youu need in nlp?\\', additional_kwargs={}, response_metadata={}, id=\\'03543f1c-e923-4845-a397-1aa71878d786\\')]\\n==================================================\\n\\n\"Attention Is All You Need\" refers to a 2017 paper that introduced the Transformer model, which revolutionized Natural Language Processing (NLP) and machine learning. The paper proposed a model that relies entirely on the mechanism of attention, without using any recurrence or convolution, to efficiently handle long-range dependencies and process input in parallel. The idea is that attention, which allows the model to dynamically focus on different parts of the input sequence, is all you need to build powerful sequence processing models. In other words, the paper argues that attention mechanisms can replace traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in NLP tasks, such as language translation, and achieve better results with less computational complexity.\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1d8dd482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Final Report: \n",
      "==================================================\n",
      "topic: [HumanMessage(content='what is attention is all youu need in nlp?', additional_kwargs={}, response_metadata={}, id='03543f1c-e923-4845-a397-1aa71878d786')]\n",
      "==================================================\n",
      "\n",
      "\"Attention Is All You Need\" refers to a 2017 paper that introduced the Transformer model, which revolutionized Natural Language Processing (NLP) and machine learning. The paper proposed a model that relies entirely on the mechanism of attention, without using any recurrence or convolution, to efficiently handle long-range dependencies and process input in parallel. The idea is that attention, which allows the model to dynamically focus on different parts of the input sequence, is all you need to build powerful sequence processing models. In other words, the paper argues that attention mechanisms can replace traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in NLP tasks, such as language translation, and achieve better results with less computational complexity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response[\"final_report\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
