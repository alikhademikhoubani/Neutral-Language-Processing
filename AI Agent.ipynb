{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLqmwDh5NSgt",
        "outputId": "5668e482-1945-494d-a548-84b7a9eecf8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.23)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.23)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Requirement already satisfied: youtube-search-python in /usr/local/lib/python3.12/dist-packages (1.6.6)\n",
            "Requirement already satisfied: httpx>=0.14.2 in /usr/local/lib/python3.12/dist-packages (from youtube-search-python) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.2->youtube-search-python) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.2->youtube-search-python) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.2->youtube-search-python) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.14.2->youtube-search-python) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.14.2->youtube-search-python) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.14.2->youtube-search-python) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.14.2->youtube-search-python) (4.15.0)\n",
            "Requirement already satisfied: youtube-search in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-search) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-search) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-search) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-search) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-search) (2025.8.3)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain_groq) (0.3.75)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain_groq) (0.31.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.15.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (0.4.23)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain_groq) (25.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.30.0->langchain_groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.30.0->langchain_groq) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.72->langchain_groq) (2.5.0)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.5)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=e077df6b7c28c7485a3a1a1b32334a9b1cd249225db5933646ded2399d246626\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community\n",
        "!pip install langchain\n",
        "!pip install wikipedia\n",
        "!pip install youtube-search-python\n",
        "!pip install youtube-search\n",
        "!pip install langchain_groq\n",
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.tools.youtube.search import YouTubeSearchTool\n",
        "from langchain_community.utilities import ArxivAPIWrapper\n",
        "from langchain_community.tools import ArxivQueryRun\n",
        "from langchain.agents import initialize_agent, AgentType"
      ],
      "metadata": {
        "id": "zZR-osGCNbJ4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model = \"llama-3.3-70b-versatile\",\n",
        "    groq_api_key = \"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\",\n",
        "    temperature = 0\n",
        ")"
      ],
      "metadata": {
        "id": "Gy-FTaORNp5_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_tool = TavilySearchResults(max_results = 1, tavily_api_key = \"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\")\n",
        "wiki_wrapper = WikipediaAPIWrapper(lang = \"fa\", top_k_results = 1)\n",
        "wiki_tool = WikipediaQueryRun(api_wrapper = wiki_wrapper)\n",
        "youtube_tool = YouTubeSearchTool()\n",
        "arxiv_wrapper = ArxivAPIWrapper(top_k_results = 1)\n",
        "arxiv_tool = ArxivQueryRun(api_wrapper = arxiv_wrapper)"
      ],
      "metadata": {
        "id": "E4vKr44jN2Y9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [tavily_tool, wiki_tool, youtube_tool, arxiv_tool]"
      ],
      "metadata": {
        "id": "Gg-kwdtSRgIw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm = llm,\n",
        "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors = True,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xsQ7y6vUFoL",
        "outputId": "cc074668-4a94-45c9-eab7-061bac7c3914"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1963084327.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"تیم فوتبال استقلال تهران کدام بازیکن را به تازگی خریداری کرده است؟\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMeyl1Z3UyYW",
        "outputId": "1bcc5c8e-8a35-4253-e7f3-f46c8f7ba19b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find the latest information about the Esteghlal Tehran football team and their recent player purchases, I should search for current events related to the team.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: Esteghlal Tehran recent player purchases\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[{'title': 'Esteghlal FC - Detailed squad 25/26 - Transfermarkt', 'url': 'https://www.transfermarkt.us/esteghlal-fc/kader/verein/1076', 'content': '| # | Player | Age | Nat. | Contract | Market value |\\n ---  ---  --- |\\n| 76 | Zob Ahan Esfahan  |  |  |  --- | | Habib Far Abbasi | Habib Far Abbasi | | Goalkeeper | | 28 | Iran | Jun 30, 2027 | €500k |\\n| 1 | Without Club  |  |  |  --- | | Antonio Adán | Antonio Adán | | Goalkeeper | | 38 | Spain | Jun 30, 2026 | €200k |\\n| 60 | Esteghlal FC U21  |  |  |  --- | | Seyed Aria Hosseinifard | Seyed Aria Hosseinifard | | Goalkeeper | | 20 | Iran  - | [...] | 7 | |  |  |  --- | | Mohammadreza Azadi | Mohammadreza Azadi | | Centre-Forward | | 25 | Iran | Jun 30, 2027 | €500k |\\n| 93 | |  |  |  --- | | Hesam Eskandari | Hesam Eskandari | | Centre-Forward | | 21 | Iran  €50k | [...] | 66 | |  |  |  --- | | Zargham Saadavi | Zargham Saadavi | | Defensive Midfield | | 19 | Iran | Jun 30, 2028 | €75k |\\n| 8 | Gol Gohar Sirjan FC  |  |  |  --- | | Amirmohammad Razzaghinia | Amirmohammad Razzaghinia | | Central Midfield | | 19 | Iran | Jun 30, 2028 | €600k |\\n| 15 | |  |  |  --- | | Abolfazl Zamani | Abolfazl Zamani | | Central Midfield | | 19 | Iran | Jun 30, 2027 | €200k |', 'score': 0.7803451}]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe search results provide information about the Esteghlal Tehran football team's current squad, but they do not specifically mention a recent player purchase. To find the latest information, I should try searching for news articles about the team's recent transfers.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: Esteghlal Tehran latest transfer news\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[{'title': 'Esteghlal New Signings Djenepo and Asani Arrive in Tehran', 'url': 'https://www.tasnimnews.com/en/news/2025/08/23/3382751/esteghlal-new-signings-djenepo-and-asani-arrive-in-tehran', 'content': \"# Esteghlal New Signings Djenepo and Asani Arrive in Tehran\\n\\nEsteghlal New Signings Djenepo and Asani Arrive in Tehran\\n\\n### TEHRAN (Tasnim) – Moussa Djenepo and Jasir Fadil Asani, who have signed deals with Esteghlal, arrived in Tehran, capital of Iran Friday afternoon.\\n\\nDjenepo, 27, has penned a two-year deal with the Iranian team.\\n\\nHe played for the Turkish side Antalyaspor last season.\\n\\nAlbanian striker Asani has also joined Esteghlal on an 18-month deal. [...] Asani, 30, played for the South Korean club Gwangju FC last season.\\n\\nMoussa Djenepo Joins Iran&apos;s Esteghlal\\n\\n##### Moussa Djenepo Joins Iran's Esteghlal\\n\\n##### Esteghlal Cancels Contract with Issa Alekasir\\n\\nMoussa Djenepo Joins Iran&apos;s Esteghlal\\n\\n##### Moussa Djenepo Joins Iran's Esteghlal\\n\\n##### Esteghlal Cancels Contract with Issa Alekasir\\n\\n##### Iran Wins Two Golds at 2025 World Wushu Championships\\n\\n##### Iran Wins Two Golds at 2025 World Wushu Championships\", 'score': 0.79542017}]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "\n",
            "Final Answer: تیم فوتبال استقلال تهران بازیکنان جدیدی مانند موزا جنپو و جاسیر فاضل آسانی را به تازگی خریداری کرده است.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'تیم فوتبال استقلال تهران کدام بازیکن را به تازگی خریداری کرده است؟',\n",
              " 'output': 'تیم فوتبال استقلال تهران بازیکنان جدیدی مانند موزا جنپو و جاسیر فاضل آسانی را به تازگی خریداری کرده است.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"فدراسیون فوتبال ایران در چه سالی تاسیس شد؟\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cERzav6HVIwA",
        "outputId": "23f8d720-5884-427a-9df3-d61ceb0d59d1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, I need to find information about the establishment of the Football Federation of Iran.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: Football Federation of Iran\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: رامین رضاییان\n",
            "Summary: رامین رضاییان سمسکندی (زادهٔ ۱ فروردین ۱۳۶۹) بازیکن فوتبال اهل ایران است که در پست دفاع راست برای باشگاه فوتبال استقلال تهران در لیگ برتر خلیج فارس بازی می‌کند.\n",
            "وی سابقه عضویت در تیم‌های صبای قم، راه‌آهن، پرسپولیس، اوستنده، الشحانیه، الدحیل، السیلیه، سپاهان و استقلال را دارد.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the observation from the wikipedia action is not relevant to the question about the establishment of the Football Federation of Iran. \n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: Football Federation Islamic Republic of Iran\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: جام جهانی فوتبال ۲۰۲۲\n",
            "Summary: جام جهانی فوتبال ۲۰۲۲ (به عربی: كأس العالم لكرة القدم ٢٠٢٢)، (به انگلیسی: FIFA World Cup 2022) بیست‌ودومین دورهٔ جام جهانی فوتبال بود که از ۲۰ نوامبر تا ۱۸ دسامبر ۲۰۲۲ (۲۹ آبان تا ۲۷ آذر ۱۴۰۱) در قطر برگزار شد. این رویداد برای اولین بار در یک کشور عربی و دومین بار در قاره آسیا (پس از جام جهانی ۲۰۰۲) برگزار شد. آرژانتین با پیروزی در فینال، عنوان قهرمانی این دوره را کسب کرد.\n",
            "این تورنمنت آخرین دوره از مسابقات بود که ۳۲ تیم در آن شرکت کردند و برای جام جهانی ۲۰۲۶ و پس از آن، حضور ۴۸ تیم برنامه‌ریزی شده‌است. با توجه به گرمای شدید قطر در تابستان، این جام جهانی از اواخر نوامبر تا اواسط دسامبر، در هشت ورزشگاه و در پنج شهر برگزار شد و اولین دوره از این رقابت‌ها بود که در ماه مه، ژوئن یا ژوئیه برگزار نشد. دیدار افتتاحیه بین قطر و اکوادور در ورزشگاه البیت برگزار شد. قطر در اولین حضور خود در جام جهانی، ۲–۰ شکست خورد و اولین کشور میزبانی شد که بازی افتتاحیهٔ خود را باخته‌است. قطر پس از شکست در دو بازی ابتدایی خود حذف شد و سریع‌ترین میزبانی شد که از مسابقات حذف شده‌است، و دومین کشور میزبان که نتوانسته‌است مرحلهٔ گروهی را پشت سر بگذارد.\n",
            "در مرحله اول مسابقات، تیم‌ها در هشت گروه چهار تیمی برای کسب امتیاز به رقابت پرداختند و دو تیم برتر هر گروه به مرحله حذفی رسیدند؛ پس از سه دور بازی، تیم‌های آرژانتین و فرانسه به فینال مسابقات راه یافتند که در ۱۸ دسامبر ۲۰۲۲، مقارن با روز ملی قطر برگزار شد. آرژانتین در فینال مقابل فرانسه با نتیجه ۴–۲ در ضربات پنالتی و پس از تساوی ۳–۳ در وقت‌های اضافه پیروز و قهرمان شد. کیلیان امباپه، بازیکن فرانسوی، پس از جف هرست در فینال ۱۹۶۶، موفق شد در فینال جام جهانی هت تریک کند و کفش طلا را به خاطر زدن بیشترین گل (۸) در طول مسابقات به دست آورد. همچنین لیونل مسی کاپیتان آرژانتین به‌عنوان بهترین بازیکن جام انتخاب شد و توپ طلا مسابقات را گرفت.\n",
            "مسابقات این دوره پر از شگفتی بود. قهرمانان دوره‌های گذشته جام‌جهانی آلمان و اروگوئه هر دو در مرحله گروهی حذف شدند - که آلمان برای دومین دوره متوالی نتوانست از مرحله گروهی صعود کند. همچنین تیم سوم مسابقات ۲۰۱۸، بلژیک در مرحله گروهی حذف شد. آرژانتین که از ابتدا از گزینه‌های قهرمانی بود، در بازی نخست خود از عربستان شکست خورد و آن‌ها را به دومین کشوری تبدیل کرد که پس از شکست در بازی اول خود، قهرمان جام جهانی شدند. برزیل، قهرمان پنج دوره جا‌م‌جهانی، در مرحله یک‌چهارم نهایی پس از شکست در ضربات پنالتی از کرواسی، حذف شد و کرواسی نیز در پایان در جایگاه سوم مسابقات قرار گرفت. مراکش به اولین کشوری از قاره آفریقا و جهان عرب تبدیل شد که به نیمه‌نهایی مسابقات رسید و اسپانیا و پرتغال را در مسیر صعودش حذف کرد. پنج بازی در مراحل حذفی از جمله فینال، به ضربات پنالتی کشیده‌شد که یک رکورد بود؛ به عبارت دیگر تمام مسابقاتی که پس از وقت ۹۰ دقیقه قانونی به وقت اضافه رسیدند، به ضربات پنالتی ختم شدند. این مسابقات همچنین پایان انتظار ۶۴ ساله حضور دوباره در جام‌جهانی برای ولز بود، طولانی‌ترین فاصله تاریخ برای هر تیمی؛ آخرین حضور آن‌ها به سال ۱۹۵۸ بر‌می‌گشت. با به ثمر رسیدن ۱۷۲ گل، رکورد جدیدی برای بیشترین تعداد گل زده در قالب ۳۲ تیمی به ثبت رسید.\n",
            "انتخاب میزبانی جام جهانی در قطر منجر به جنجال‌های بحث‌برانگیزی به ویژه در جهان غرب شد، که برخی از آن‌ها شامل رفتار بد قطر با کارگران مهاجر، سوابق ضعیف حقوق بشر، حقوق دگرباشان جنسی و اتهامات دیگر مربوط به ورزش‌شویی است. سپ بلاتر، رئیس سابق فیفا مدعی شد که قطر برای کسب حق میزبانی تقلب کرده‌است، و دو بار اعلام کرد که دادن حق میزبانی به قطر یک «اشتباه» بود؛ در حالی که جانی اینفانتینو، رئیس وقت فیفا، از میزبانی مسابقات در قطر دفاع کرد. عده‌ای اظهار کردند که آب و هوای طاقت فرسا قطر و فقدان فرهنگ قوی فوتبالی، دلیلی بر رشوه‌دهی برای حقوق میزبانی و فساد گسترده در فیفا است. این مناقشه‌ها به‌عنوان یک تضاد فرهنگی و جنگ تمدن‌ها بین عقاید اسلام و هنجارهای سکولاریته دموکراسی آزاد غرب توصیف شد. برخی دیگر آن را نشان دهنده ژئوپلیتیک و کاهش نفوذ غرب در فوتبال دانستند.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the observation from the wikipedia action is not relevant to the question about the establishment of the Football Federation of Iran.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: Football Federation of Iran establishment\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mNo good Wikipedia Search Result was found\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the observation from the wikipedia action is not relevant to the question about the establishment of the Football Federation of Iran. I should try searching for the information in a different way.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: Iran Football Federation\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: علی علیپور\n",
            "Summary: علی علیپور (زادهٔ ۲۰ آبان ۱۳۷۴) بازیکن فوتبال اهل ایران است که در پست مهاجم برای باشگاه فوتبال پرسپولیس در لیگ برتر خلیج فارس بازی می‌کند.\n",
            "علیپور در باشگاه پرسپولیس در فصل ۹۷–۱۳۹۶ آقای گل لیگ برتر ایران و نیز در ۹۷–۱۳۹۶ بهترین مهاجم سال فوتبال ایران شد. علیپور جوان‌ترین گلزن تاریخ شهرآورد تهران است. علی علیپور موفق به زدن ۵ گل در شهرآورد تهران شده است. همچنین او در فینال جام حذفی فوتبال ایران ۹۸–۱۳۹۷ گل پیروزی‌بخش پرسپولیس را به ثمر رساند.\n",
            "علیپور سابقه بازی و گلزنی برای تیم ملی نوجوانان و جوانان ایران را دارد و همچنین برای تیم ملی بزرگسالان ایران نیز بازی کرده است.\n",
            "علیپور در سال ۱۴۰۳ بعد از چند سال دوری از فوتبال ایران به تیم فوتبال پرسپولیس بازگشت.\n",
            "همچنین او سومین گلزن برتر تاریخ پرسپولیس است.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the observation from the wikipedia action is not relevant to the question about the establishment of the Football Federation of Iran. I should try searching for the information in a different way.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: فدراسیون فوتبال ایران\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: فدراسیون فوتبال ایران\n",
            "Summary: فدراسیون فوتبال ایران (کوتاه‌شدهٔ انگلیسی: FFIRI، کد فیفا: IRN) یک نهاد غیردولتی است با نام رسمی فدراسیون فوتبال جمهوری اسلامی ایران که فوتبال، فوتسال و فوتبال ساحلی را در کشور ایران اداره می‌کند.\n",
            "فدراسیون فوتبال ایران در سال ۱۲۹۸ هجری خورشیدی (۱۹۲۰ میلادی) تأسیس شده، از سال ۱۹۴۸ عضو فیفا و از سال ۱۹۵۴ عضو ای‌اف‌سی می‌باشد. فدراسیون فوتبال ایران همچنین عضو مؤسس فدراسیون فوتبال غرب آسیا در سال ۲۰۰۱ بوده‌است، نهادی که در سال ۲۰۱۴ از عضویت آن خارج شد و با اقدام به تأسیس فدراسیون فوتبال آسیای مرکزی به عضویت آن درآمد. این نهاد، دارندهٔ قانونی برند و نماد بازرگانی تیم ملی فوتبال ایران است و این تیم را سازمان‌دهی می‌کند.\n",
            "در تاریخ ۲ آذر ۱۳۸۵ (فیفا)، فدراسیون ایران را از حضور در میدان‌های بین‌المللی محروم ساخت.\n",
            "روز دوشنبه ۲۷ مرداد ۱۳۹۹ در پی شکایت سرمربی سابق تیم ملی فوتبال، فیفا، فدراسیون فوتبال جمهوری اسلامی ایران را به پرداختن غرامت ۶ میلیون و ۱۳۷ یورویی به وی محکوم کرد و همچنین در روز ۱۰ اسفند ۱۳۹۹، شهاب الدین عزیزی خادم به سمت ریاست این فدراسیون انتخاب و در تاریخ ۲۸ بهمن ۱۴۰۰ با رای اکثریت هیئت رئیسه برکنار شد.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: فدراسیون فوتبال ایران در سال ۱۲۹۸ هجری خورشیدی (۱۹۲۰ میلادی) تأسیس شده است.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'فدراسیون فوتبال ایران در چه سالی تاسیس شد؟',\n",
              " 'output': 'فدراسیون فوتبال ایران در سال ۱۲۹۸ هجری خورشیدی (۱۹۲۰ میلادی) تأسیس شده است.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"دو ویدیو در مورد آموزش پخت چیز کیک به من ارائه کن\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiZODaJwV84m",
        "outputId": "da8dac37-b917-4100-c487-fb469e34354e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find videos about making cheesecakes, I should search for YouTube videos related to \"cheesecake recipe\" or \"how to make cheesecake\". \n",
            "\n",
            "Action: youtube_search\n",
            "Action Input: cheesecake recipe,2\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m['https://www.youtube.com/watch?v=Mfig-pIyh-g&pp=ygURY2hlZXNlY2FrZSByZWNpcGU%3D', 'https://www.youtube.com/watch?v=tspdJ6hxqnc&pp=ygURY2hlZXNlY2FrZSByZWNpcGU%3D']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have found two YouTube videos related to making cheesecakes. \n",
            "\n",
            "Action: None\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action Input:' after 'Action:'\n",
            "Thought:\u001b[32;1m\u001b[1;3mQuestion: دو ویدیو در مورد آموزش پخت چیز کیک به من ارائه کن\n",
            "Thought: To find videos about making cheesecakes, I should search for YouTube videos related to \"cheesecake recipe\" or \"how to make cheesecake\". \n",
            "\n",
            "Action: youtube_search\n",
            "Action Input: cheesecake recipe,2\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m['https://www.youtube.com/watch?v=Mfig-pIyh-g&pp=ygURY2hlZXNlY2FrZSByZWNpcGU%3D', 'https://www.youtube.com/watch?v=tpsywnPNDsw&pp=ygURY2hlZXNlY2FrZSByZWNpcGXSBwkJsgkBhyohjO8%3D']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: برای آموزش پخت چیز کیک می توانید از دو ویدیو زیر استفاده کنید: https://www.youtube.com/watch?v=Mfig-pIyh-g و https://www.youtube.com/watch?v=tpsywnPNDsw\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'دو ویدیو در مورد آموزش پخت چیز کیک به من ارائه کن',\n",
              " 'output': 'برای آموزش پخت چیز کیک می توانید از دو ویدیو زیر استفاده کنید: https://www.youtube.com/watch?v=Mfig-pIyh-g و https://www.youtube.com/watch?v=tpsywnPNDsw'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"Exploring and Mitigating Fawning Hallucinations in Large Language Models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxCNQR-FWwez",
        "outputId": "2b1ac2df-e0c9-4fd6-e8ab-d0b7c9ff9de0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo address the topic of \"Exploring and Mitigating Fawning Hallucinations in Large Language Models,\" we need to delve into the realm of artificial intelligence, specifically focusing on the challenges and limitations of large language models (LLMs). The term \"fawning hallucinations\" refers to a phenomenon where these models generate text that is overly flattering or attempts to appease the user, often at the expense of factual accuracy or relevance.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: \"fawning hallucinations in large language models\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2025-08-31\n",
            "Title: Exploring and Mitigating Fawning Hallucinations in Large Language Models\n",
            "Authors: Zixuan Shangguan, Yanjie Dong, Lanjun Wang, Xiaoyi Fan, Victor C. M. Leung, Xiping Hu\n",
            "Summary: Large language models (LLMs) have demonstrated exceptional proficiency in\n",
            "language understanding. However, when LLMs align their outputs with deceptive\n",
            "and/or misleading prompts, the generated responses could deviate from the de\n",
            "facto information. Such observations are known as fawning hallucinations, where\n",
            "the model prioritizes alignment with the input's implied perspective over\n",
            "accuracy and truthfulness. In this work, we analyze fawning hallucinations in\n",
            "various natural language processing tasks and tailor the so-termed contrastive\n",
            "decoding method for fawning-hallucination mitigation. Specifically, we design\n",
            "two paradigms to generate corresponding deceptive and/or misleading inputs for\n",
            "the consistent fawning hallucinations induction. Then, we propose the\n",
            "collaborative contrastive decoding (CCD) to handle the fawning hallucinations\n",
            "across different tasks in LLMs. By contrasting the deviation in output\n",
            "distribution between induced and transformed neutral inputs, the proposed CCD\n",
            "can reduce reliance on deceptive and/or misleading information without\n",
            "requiring additional training. Extensive experiments demonstrate that the\n",
            "proposed CCD can effectively mitigate fawning hallucinations and improve the\n",
            "factuality of the generated responses over various tasks.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The observation from the arxiv search provides a recent research paper that directly addresses the issue of \"fawning hallucinations\" in large language models. The paper proposes a method called collaborative contrastive decoding (CCD) to mitigate these hallucinations, which involves contrasting the deviation in output distribution between induced and transformed neutral inputs. This approach aims to reduce the reliance on deceptive or misleading information without requiring additional training. The experiments conducted by the authors demonstrate the effectiveness of the proposed method in mitigating fawning hallucinations and improving the factuality of the generated responses across various tasks.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: \"large language models\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: مدل زبانی بزرگ\n",
            "Summary: مدل زبانی بزرگ (اختصاری LLM) یا ال‌ال‌ام، سامانه‌های هوش مصنوعی هستند که برای درک، تولید و پاسخگویی به زبان انسان طراحی شده‌اند. آنها «بزرگ» نامیده می‌شوند زیرا حاوی میلیاردها پارامتر هستند که به آنها امکان می‌دهد الگوهای پیچیده در داده‌های زبان را پردازش کنند.\n",
            "ال‌ال‌ام‌ها دسته‌ای از مدل‌های یادگیری ماشینی هستند که توانایی درک و تولید متنی شبیه انسان را دارند. این مدل‌ها این توانایی‌ها را با یادگیری روابط آماری از اسناد متنی در طی یک فرایند آموزشی فشرده محاسباتی خود نظارت و نیمه نظارت به دست می‌آورند. آنها بر روی مجموعه داده‌های گسترده‌ای آموزش دیده‌اند که اغلب از اینترنت جمع‌آوری می‌شوند، و می‌تواند شامل منابع متنی بسیار متنوعی مانند صفحات ویکی‌پدیا، کتاب‌ها، موضوعات رسانه‌های اجتماعی و مقالات خبری باشد. مدل‌های زبان بزرگ در حدود سال ۲۰۱۸ ظاهر شدند و در گستره وسیعی از وظایف، عملکرد بالایی دارند. در معماری داخلی این مدل‌ها از ترنسفورمر بهره برده شده است. ترنسفورمرها شبکه‌های عصبی مصنوعی هستند که برای پردازش دنباله‌های طولانی توکن‌ها (نشانه‌ها) به سازوکارهای توجه متکی هستند و معمولاً از ده‌ها میلیون و تا میلیاردها پارامتر آموزش‌دیده دارند. ترنسفورمرها می‌توانند وابستگی‌ها و روابط بین کلمات و جملات، و همچنین نحو، معناشناسی و زمینه زبان طبیعی را به تصویر بکشند لذا می‌توان به عنوان نوعی هوش مصنوعی مولد، از آنها برای تولید متن بهره برد. در این صورت هوش مصنوعی مولد با گرفتن یک متن ورودی و پیش‌بینی مکرر نشانه یا کلمه بعدی متن شبه‌انسانی تولید می‌کند.\n",
            "تا تاریخ مارس ۲۰۲۴ بزرگ‌ترین و تواناترین آنها، با معماری مبتنی بر ترنسفورمرهای فقط رمزگشا ساخته شده‌اند. در عین حال، برخی دیگر از پیاده‌سازی‌ها بر اساس معماری‌هایی مانند انواع شبکه عصبی بازگشتی و مامبا (یک مدل فضای حالت) هستند.\n",
            "برخی شرکت‌های پیشرو در ارائه LLMها و محصولات آنها عبارتند از\n",
            "\n",
            "این مدل‌ها با اندازه‌ها و قابلیت‌های متفاوتی ارائه می‌شوند که از بین ده‌ها میلیون تا میلیاردها پارامتر متغیر است. پارامترها وزن‌هایی مدل هستند که در طول آموزش یادمی‌گیرد نشانه یا کلمه بعدی را در یک دنباله پیش‌بینی کنند.\n",
            "ال‌ال‌ام‌ها را می‌توان برای کارهای مختلف پردازش زبان طبیعی (اختصاری NLP) مانند تولید متن، خلاصه‌سازی متن، پرسش و پاسخ، ترجمه ماشینی، تجزیه و تحلیل احساسات، طبقه‌بندی متن و موارد دیگر استفاده کرد.\n",
            "ال‌ال‌ام‌ها همچنین می‌توانند تنظیم دقیق آنها در مجموعه داده‌های کوچکتر یا استفاده از شگردهای مهندسی پرسش، با دامنه‌ها یا وظایف خاص سازگار شوند. با این حال، ال‌ال‌ام‌ها نیز دارای محدودیت‌ها و چالش‌هایی هستند. به عنوان مثال، ال‌ال‌ام‌ها ممکن است نادرستی و سوگیری را از داده‌هایی که بر روی آنها آموزش دیده‌اند یا از روش‌های مهندسی سریع خود به ارث ببرند. ال‌ال‌ام‌ها همچنین ممکن است محتوای مضر یا گمراه‌کننده تولید کنند که می‌تواند بر رفاه انسان یا هنجارهای اجتماعی تأثیر بگذارد؛ بنابراین، ال‌ال‌ام‌ها باید به دقت ارزیابی شوند و توسط توسعه دهندگان و کاربران به‌طور مسئولانه استفاده شوند.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The observation from the wikipedia search provides a comprehensive overview of large language models (LLMs), including their architecture, training process, and applications. It also highlights the limitations and challenges of LLMs, such as the potential for bias, inaccuracies, and generation of harmful or misleading content. This information provides a broader context for understanding the issue of \"fawning hallucinations\" in LLMs and the importance of developing methods to mitigate these hallucinations.\n",
            "\n",
            "Action: tavily_search_results_json\n",
            "Action Input: \"fawning hallucinations in AI\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[{'title': 'Hallucination (artificial intelligence)', 'url': 'https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)', 'content': 'In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called confabulation, or delusion) is a response generated by AI that contains false or misleading information presented as fact. This term draws a loose analogy with human psychology, where a hallucination typically involves false percepts. However, there is a key difference: AI hallucination is associated with erroneously constructed responses (confabulation), rather than perceptual experiences. [...] Various researchers cited by Wired \"Wired (magazine)\") have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some \"incorrect\" AI responses classified by humans as \"hallucinations\" in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the \"correct\" answer that the human reviewers are failing to see. For', 'score': 0.76173395}]\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: The observation from the tavily_search_results_json search provides additional information on the concept of \"hallucinations\" in artificial intelligence, including the definition, causes, and implications of this phenomenon. The search results also highlight the complexity of addressing hallucinations in AI, as some researchers argue that certain \"incorrect\" responses may be justified by the training data or may even be the correct answer that human reviewers are failing to see.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Exploring and mitigating fawning hallucinations in large language models is an active area of research, with recent studies proposing methods such as collaborative contrastive decoding (CCD) to reduce the reliance on deceptive or misleading information. Large language models are powerful tools for natural language processing, but they can also generate harmful or misleading content, highlighting the need for responsible development and use. The concept of hallucinations in AI is complex and multifaceted, and addressing it requires a deep understanding of the underlying causes and implications.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Exploring and Mitigating Fawning Hallucinations in Large Language Models',\n",
              " 'output': 'Exploring and mitigating fawning hallucinations in large language models is an active area of research, with recent studies proposing methods such as collaborative contrastive decoding (CCD) to reduce the reliance on deceptive or misleading information. Large language models are powerful tools for natural language processing, but they can also generate harmful or misleading content, highlighting the need for responsible development and use. The concept of hallucinations in AI is complex and multifaceted, and addressing it requires a deep understanding of the underlying causes and implications.'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"give me a summary of Exploring and Mitigating Fawning Hallucinations in Large Language Models\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU7UGElYXavr",
        "outputId": "4009d7da-4c05-4b11-d7fc-c0b49f415a4d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo provide a summary of \"Exploring and Mitigating Fawning Hallucinations in Large Language Models\", I need to understand what this topic is about, which seems to be related to artificial intelligence and natural language processing. \n",
            "\n",
            "Action: arxiv\n",
            "Action Input: Exploring and Mitigating Fawning Hallucinations in Large Language Models\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2025-08-31\n",
            "Title: Exploring and Mitigating Fawning Hallucinations in Large Language Models\n",
            "Authors: Zixuan Shangguan, Yanjie Dong, Lanjun Wang, Xiaoyi Fan, Victor C. M. Leung, Xiping Hu\n",
            "Summary: Large language models (LLMs) have demonstrated exceptional proficiency in\n",
            "language understanding. However, when LLMs align their outputs with deceptive\n",
            "and/or misleading prompts, the generated responses could deviate from the de\n",
            "facto information. Such observations are known as fawning hallucinations, where\n",
            "the model prioritizes alignment with the input's implied perspective over\n",
            "accuracy and truthfulness. In this work, we analyze fawning hallucinations in\n",
            "various natural language processing tasks and tailor the so-termed contrastive\n",
            "decoding method for fawning-hallucination mitigation. Specifically, we design\n",
            "two paradigms to generate corresponding deceptive and/or misleading inputs for\n",
            "the consistent fawning hallucinations induction. Then, we propose the\n",
            "collaborative contrastive decoding (CCD) to handle the fawning hallucinations\n",
            "across different tasks in LLMs. By contrasting the deviation in output\n",
            "distribution between induced and transformed neutral inputs, the proposed CCD\n",
            "can reduce reliance on deceptive and/or misleading information without\n",
            "requiring additional training. Extensive experiments demonstrate that the\n",
            "proposed CCD can effectively mitigate fawning hallucinations and improve the\n",
            "factuality of the generated responses over various tasks.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have found a research paper that matches the topic I'm looking for, which provides a summary of the concept of \"Exploring and Mitigating Fawning Hallucinations in Large Language Models\". The paper discusses how large language models can generate responses that deviate from factual information when aligned with deceptive or misleading prompts, and proposes a method called collaborative contrastive decoding (CCD) to mitigate this issue.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: The paper \"Exploring and Mitigating Fawning Hallucinations in Large Language Models\" discusses the issue of large language models generating responses that prioritize alignment with the input's implied perspective over accuracy and truthfulness, and proposes a method called collaborative contrastive decoding (CCD) to mitigate this issue. The CCD method can reduce reliance on deceptive and/or misleading information without requiring additional training, and can effectively improve the factuality of the generated responses over various tasks.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'give me a summary of Exploring and Mitigating Fawning Hallucinations in Large Language Models',\n",
              " 'output': 'The paper \"Exploring and Mitigating Fawning Hallucinations in Large Language Models\" discusses the issue of large language models generating responses that prioritize alignment with the input\\'s implied perspective over accuracy and truthfulness, and proposes a method called collaborative contrastive decoding (CCD) to mitigate this issue. The CCD method can reduce reliance on deceptive and/or misleading information without requiring additional training, and can effectively improve the factuality of the generated responses over various tasks.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"transformers in computer vision\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlWpkUMOYMJX",
        "outputId": "fe2ac37f-8a87-4362-b61e-c88950a2ed34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer questions about transformers in computer vision, I should first look for general information on the topic to understand the context and current state of research.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: transformers in computer vision\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: هوش مصنوعی\n",
            "Summary: هوش مصنوعی (به انگلیسی: Artificial intelligence) (مخفف انگلیسی: AI) هوشی است که توسط ماشین‌ها ظهور پیدا می‌کند، در مقابل هوش طبیعی که توسط جانوران شامل انسان‌ها نمایش می‌یابد. اما پیش از هرچیز باید این موضوع را دانست که کلمه هوش، نشان دهنده امکان استدلال است و اینکه آیا هوش مصنوعی می‌تواند به توانایی استدلال دست یابد یا خیر، خود موضوع اختلاف محققان است.\n",
            "کتاب‌های AI پیشرو، این شاخه را به عنوان شاخه مطالعه بر روی «عوامل هوشمند» تعریف می‌کنند: هر سامانه‌ای که محیط خود را درک کرده و کنش‌هایی را انجام می‌دهد که شانسش را در دستیابی به اهدافش بیشینه می‌سازد. برخی از منابع شناخته شده از اصطلاح «هوش مصنوعی» جهت توصیف ماشینی استفاده می‌کنند که عملکردهای «شناختی» را از روی ذهن انسان‌ها تقلید می‌کنند، همچون «یادگیری» و «حل مسئله»، با این حال این تعریف توسط محققان اصلی در زمینه AI رد شده است.\n",
            "کاربردهای هوش مصنوعی شامل موتورهای جستجو پیشرفتهٔ وب (مثل گوگل و بینگ)، سامانه توصیه‌گر (که توسط یوتیوب، آمازون و نتفلیکس استفاده می‌شوند)، فهم زبان انسان‌ها (همچون چت‌جی‌پی‌تی، جیمنای و گراک)، خودروهای خودران (مثل تسلا)، هوش مصنوعی مولد یا خلاقیت محاسباتی (مثل چت‌جی‌پی‌تی یا تولید اثر هنری مانند دال-ئی و میدجرنی) تصمیم‌گیری خودکار و رقابت در بالاترین سطوح سامانه‌های بازی استراتژیک (همچون شطرنج و گو). با بیشتر شدن توانایی ماشین‌ها، وظایفی که نیازمند «هوشمندی» هستند اغلب از تعریف AI برداشته می‌شود، پدیده‌ای که به آن اثر هوش مصنوعی گفته می‌شود. به عنوان مثال، فهم نوری کاراکتر را اغلب از چیزهایی که AI در نظر گرفته می‌شوند مستثنی می‌کنند، چرا که این فناوری تبدیل به فناوری عادی و روزمره‌ای شده است. (استفاده از هوش مصنوعی در زمینه‌هایی مانند پزشکی و آموزش رو به افزایش است).\n",
            "هوش مصنوعی در سال ۱۹۵۶ میلادی تبدیل به شاخه‌ای آکادمیک شد و در سال‌های پس از آن چندین موج خوش‌بینی را تجربه کرده و مجدد دچار امواج ناامیدی و کمبود بودجه شده (که به آن «زمستان AI» می‌گویند)، سپس فناوری‌های جدیدی در پی آن آمده و موفقیت و بودجه‌های تحقیقاتی این حوزه مجدداً احیا گشته‌اند. تحقیقات AI رهیافت‌های متفاوتی را از زمان تأسیسش امتحان کرده و آن‌ها را کنار گذاشته است، رهیافت‌هایی چون: شبیه‌سازی مغز، مدل‌سازی حل مسئله توسط مغز انسان، منطق صوری، بانک‌های اطلاعاتی بزرگ دانش و تقلید رفتار جانوران. در اولین دهه‌های قرن ۲۱ میلادی، یادگیری ماشینی که شدیداً از آمار ریاضیاتی بهره می‌برد در این حوزه غلبه داشت و این فناوری اثبات کرد که به شدت موفق است و به حل چندین مسئله چالش‌برانگیز در صنعت و فضای آکادمیک کمک نمود.\n",
            "شاخه‌های مختلف تحقیقات هوش مصنوعی حول اهداف به‌خصوصی متمرکز بوده و از ابزارآلات خاصی استفاده می‌کنند. اهداف سنتی تحقیقات AI شامل این موارد اند: استدلال، نمایش دانش، برنامه‌ریزی، یادگیری، پردازش زبان طبیعی، ادراک و توانایی در جابجایی و دستکاری اشیاء. هوش جامع (توانایی حل مسائل دلخواه) در میان اهداف بلند مدت این حوزه است. جهت حل چنین مسائلی، محققان AI فنون حل مسئله وسیع و یکپارچه‌ای را شامل این موارد به کار بسته‌اند: جست‌وجو و بهینه‌سازی ریاضیاتی، منطق صوری، شبکه‌های عصبی مصنوعی و روش‌های مبنی بر آمار، احتمالات و اقتصاد. AI همچنین با حوزه‌هایی چون علوم کامپیوتر، روان‌شناسی، زبان‌شناسی، فلسفه و بسیاری از حوزه‌های دیگر مرتبط است.\n",
            "این شاخه بر این فرض بنا شده است که هوش انسانی «را می‌توان به دقت توصیف نمود، به طوری که می‌توان آن را توسط یک ماشین شبیه‌سازی نمود». این فرض بحث‌های فلسفی را پیرامون ذهن و اخلاقیات خلق موجودات هوشمند برانگیخته است، موجوداتی که دارای هوش شبیه-انسان اند. این مسائل توسط افسانه‌ها، داستان‌های تخیلی و فلسفه از زمان‌های باستان مورد کاوش واقع شده‌اند. ادبیات علمی-تخیلی و آینده‌پژوهی نیز پیشنهاد می‌دهند که AI با پتانسیل و قدرت عظیمی که دارد، ممکن است منجر به ایجاد ریسک وجودی برای بشریت گردد.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the Wikipedia search results are not relevant to the topic of transformers in computer vision. The results appear to be about artificial intelligence in general.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: transformers in computer vision\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-07-27\n",
            "Title: A survey of the Vision Transformers and their CNN-Transformer based Variants\n",
            "Authors: Asifullah Khan, Zunaira Rauf, Anabia Sohail, Abdul Rehman, Hifsa Asif, Aqsa Asif, Umair Farooq\n",
            "Summary: Vision transformers have become popular as a possible substitute to\n",
            "convolutional neural networks (CNNs) for a variety of computer vision\n",
            "applications. These transformers, with their ability to focus on global\n",
            "relationships in images, offer large learning capacity. However, they may\n",
            "suffer from limited generalization as they do not tend to model local\n",
            "correlation in images. Recently, in vision transformers hybridization of both\n",
            "the convolution operation and self-attention mechanism has emerged, to exploit\n",
            "both the local and global image representations. These hybrid vision\n",
            "transformers, also referred to as CNN-Transformer architectures, have\n",
            "demonstrated remarkable results in vision applications. Given the rapidly\n",
            "growing number of hybrid vision transformers, it has become necessary to\n",
            "provide a taxonomy and explanation of these hybrid architectures. This survey\n",
            "presents a taxonomy of the recent vision transformer architectures and more\n",
            "specifically that of the hybrid vision transformers. Additionally, the key\n",
            "features of these architectures such as the attention mechanisms, positional\n",
            "embeddings, multi-scale processing, and convolution are also discussed. In\n",
            "contrast to the previous survey papers that are primarily focused on individual\n",
            "vision transformer architectures or CNNs, this survey uniquely emphasizes the\n",
            "emerging trend of hybrid vision transformers. By showcasing the potential of\n",
            "hybrid vision transformers to deliver exceptional performance across a range of\n",
            "computer vision tasks, this survey sheds light on the future directions of this\n",
            "rapidly evolving architecture.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the arxiv search results are relevant to the topic of transformers in computer vision. The results appear to be about a survey of vision transformers and their CNN-Transformer based variants.\n",
            "\n",
            "Action: arxiv\n",
            "Action Input: vision transformers applications\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-07-27\n",
            "Title: A survey of the Vision Transformers and their CNN-Transformer based Variants\n",
            "Authors: Asifullah Khan, Zunaira Rauf, Anabia Sohail, Abdul Rehman, Hifsa Asif, Aqsa Asif, Umair Farooq\n",
            "Summary: Vision transformers have become popular as a possible substitute to\n",
            "convolutional neural networks (CNNs) for a variety of computer vision\n",
            "applications. These transformers, with their ability to focus on global\n",
            "relationships in images, offer large learning capacity. However, they may\n",
            "suffer from limited generalization as they do not tend to model local\n",
            "correlation in images. Recently, in vision transformers hybridization of both\n",
            "the convolution operation and self-attention mechanism has emerged, to exploit\n",
            "both the local and global image representations. These hybrid vision\n",
            "transformers, also referred to as CNN-Transformer architectures, have\n",
            "demonstrated remarkable results in vision applications. Given the rapidly\n",
            "growing number of hybrid vision transformers, it has become necessary to\n",
            "provide a taxonomy and explanation of these hybrid architectures. This survey\n",
            "presents a taxonomy of the recent vision transformer architectures and more\n",
            "specifically that of the hybrid vision transformers. Additionally, the key\n",
            "features of these architectures such as the attention mechanisms, positional\n",
            "embeddings, multi-scale processing, and convolution are also discussed. In\n",
            "contrast to the previous survey papers that are primarily focused on individual\n",
            "vision transformer architectures or CNNs, this survey uniquely emphasizes the\n",
            "emerging trend of hybrid vision transformers. By showcasing the potential of\n",
            "hybrid vision transformers to deliver exceptional performance across a range of\n",
            "computer vision tasks, this survey sheds light on the future directions of this\n",
            "rapidly evolving architecture.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mIt seems like the arxiv search results are still relevant to the topic of transformers in computer vision, and the survey paper provides a comprehensive overview of the recent vision transformer architectures and their applications.\n",
            "\n",
            "Thought: I now know the final answer\n",
            "\n",
            "Final Answer: Transformers in computer vision refer to a type of neural network architecture that has become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity, but may suffer from limited generalization as they do not tend to model local correlation in images. Recently, hybrid vision transformers, also referred to as CNN-Transformer architectures, have emerged, which combine the convolution operation and self-attention mechanism to exploit both local and global image representations, and have demonstrated remarkable results in vision applications. The survey paper provides a taxonomy of the recent vision transformer architectures and their CNN-Transformer based variants, and discusses the key features of these architectures, such as attention mechanisms, positional embeddings, multi-scale processing, and convolution.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'transformers in computer vision',\n",
              " 'output': 'Transformers in computer vision refer to a type of neural network architecture that has become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity, but may suffer from limited generalization as they do not tend to model local correlation in images. Recently, hybrid vision transformers, also referred to as CNN-Transformer architectures, have emerged, which combine the convolution operation and self-attention mechanism to exploit both local and global image representations, and have demonstrated remarkable results in vision applications. The survey paper provides a taxonomy of the recent vision transformer architectures and their CNN-Transformer based variants, and discusses the key features of these architectures, such as attention mechanisms, positional embeddings, multi-scale processing, and convolution.'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.invoke(\"علی دایی چه کسی است؟ دو ویدیو از گلزنی های او را به من ارائه کن.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF-BvWwlYeN2",
        "outputId": "3d7dd48f-b0b6-4f24-fb50-528b16c1fb7f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: علی دایی یک بازیکن فوتبال ایرانی است و من باید اطلاعات بیشتری در مورد او و گلزنی هایش پیدا کنم. برای این کار، من می توانم از ویکی پدیا برای پیدا کردن اطلاعات در مورد او استفاده کنم و سپس از یوتیوب برای پیدا کردن ویدیوهای گلزنی هایش استفاده کنم.\n",
            "\n",
            "Action: wikipedia\n",
            "Action Input: علی دایی\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: علی دایی\n",
            "Summary: علی دایی (زادهٔ ۱ فروردین ۱۳۴۸) بازیکن بازنشسته و مربی فوتبال اهل ایران است. دایی در ردهٔ باشگاهی، علاوه بر بازی در لیگ ایران، سابقهٔ حضور در لیگ‌های قطر، آلمان و امارات را نیز داشته است.\n",
            "دایی با زدن ۱۰۹ گل در ۱۴۸ بازی ملی بهترین گلزن تاریخ تیم ملی فوتبال ایران و سومین گلزن برتر جهان در ردهٔ ملی، شناخته می‌شود. دایی به‌ عنوان اولین بازیکن جهان محسوب می‌شود که بیش از صد گل ملی به ثمر رسانده‌است. او در نظرسنجی سایت کنفدراسیون فوتبال آسیا و برنامهٔ تلویزیونی ۹۰ به ترتیب با کسب «۵۹ درصد» و «۷۳ درصد» آراء مخاطبان، به‌ عنوان بهترین مهاجم تاریخ جام ملت‌های آسیا و بهترین مهاجم بعد از انقلاب ۱۳۵۷ در ایران انتخاب شد.‏ دایی با ثبت ۱۴ گل، رکورد بیشترین گلزنی در تمام ادوار جام ملت‌های آسیا را در اختیار دارد.\n",
            "دایی در سال ۱۳۸۵ به عنوان بازیکن-مربی باشگاه سایپا، پا به عرصهٔ مربیگری گذاشت. او در اسفند ۱۳۸۶ نیز به عنوان سرمربی تیم ملی ایران انتخاب شد.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: با استفاده از اطلاعات ویکی پدیا، من فهمیدم که علی دایی یک بازیکن فوتبال ایرانی است که رکورددار گلزنی در تیم ملی ایران است. حالا، من می توانم از یوتیوب برای پیدا کردن ویدیوهای گلزنی هایش استفاده کنم.\n",
            "\n",
            "Action: youtube_search\n",
            "Action Input: علی دایی, 2\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m['https://www.youtube.com/shorts/Ln6h9n1nfo4', 'https://www.youtube.com/watch?v=Rs1oVlSJMTE&pp=ygUP2LnZhNuMINiv2KfbjNuM']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThought: با استفاده از اطلاعات یوتیوب، من توانستم دو ویدیو از گلزنی های علی دایی را پیدا کنم. حالا، من می توانم پاسخ نهایی را ارائه کنم.\n",
            "\n",
            "Final Answer: علی دایی یک بازیکن فوتبال ایرانی است که رکورددار گلزنی در تیم ملی ایران است. دو ویدیو از گلزنی های او در لینک های زیر موجود است: https://www.youtube.com/shorts/Ln6h9n1nfo4 و https://www.youtube.com/watch?v=Rs1oVlSJMTE&pp=ygUP2LnZhNuMINiv2KfbjNuM\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'علی دایی چه کسی است؟ دو ویدیو از گلزنی های او را به من ارائه کن.',\n",
              " 'output': 'علی دایی یک بازیکن فوتبال ایرانی است که رکورددار گلزنی در تیم ملی ایران است. دو ویدیو از گلزنی های او در لینک های زیر موجود است: https://www.youtube.com/shorts/Ln6h9n1nfo4 و https://www.youtube.com/watch?v=Rs1oVlSJMTE&pp=ygUP2LnZhNuMINiv2KfbjNuM'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}