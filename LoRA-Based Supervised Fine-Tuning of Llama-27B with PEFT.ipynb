{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q peft transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-04T11:57:55.162969Z","iopub.execute_input":"2025-06-04T11:57:55.163120Z","iopub.status.idle":"2025-06-04T11:59:14.739956Z","shell.execute_reply.started":"2025-06-04T11:57:55.163105Z","shell.execute_reply":"2025-06-04T11:59:14.739201Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:15:02.400372Z","iopub.execute_input":"2025-06-04T12:15:02.400655Z","iopub.status.idle":"2025-06-04T12:15:07.962985Z","shell.execute_reply.started":"2025-06-04T12:15:02.400623Z","shell.execute_reply":"2025-06-04T12:15:07.962300Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:15:22.262333Z","iopub.execute_input":"2025-06-04T12:15:22.262945Z","iopub.status.idle":"2025-06-04T12:15:25.854362Z","shell.execute_reply.started":"2025-06-04T12:15:22.262913Z","shell.execute_reply":"2025-06-04T12:15:25.853703Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install sentencepiece","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:15:42.759149Z","iopub.execute_input":"2025-06-04T12:15:42.759434Z","iopub.status.idle":"2025-06-04T12:15:45.643267Z","shell.execute_reply.started":"2025-06-04T12:15:42.759382Z","shell.execute_reply":"2025-06-04T12:15:45.642621Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import (\nAutoTokenizer,\nAutoModelForCausalLM,\nBitsAndBytesConfig,\npipeline,\nlogging,\nHfArgumentParser,\nTrainingArguments\n)\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:36:08.927053Z","iopub.execute_input":"2025-06-04T13:36:08.927569Z","iopub.status.idle":"2025-06-04T13:36:08.931390Z","shell.execute_reply.started":"2025-06-04T13:36:08.927536Z","shell.execute_reply":"2025-06-04T13:36:08.930689Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"!pip install -q -U einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T12:16:10.707166Z","iopub.execute_input":"2025-06-04T12:16:10.707721Z","iopub.status.idle":"2025-06-04T12:16:13.615735Z","shell.execute_reply.started":"2025-06-04T12:16:10.707691Z","shell.execute_reply":"2025-06-04T12:16:13.614965Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:22.515078Z","iopub.execute_input":"2025-06-04T13:42:22.515366Z","iopub.status.idle":"2025-06-04T13:42:22.519266Z","shell.execute_reply.started":"2025-06-04T13:42:22.515344Z","shell.execute_reply":"2025-06-04T13:42:22.518538Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"logging.set_verbosity_warning()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:25.189196Z","iopub.execute_input":"2025-06-04T13:42:25.189490Z","iopub.status.idle":"2025-06-04T13:42:25.193815Z","shell.execute_reply.started":"2025-06-04T13:42:25.189469Z","shell.execute_reply":"2025-06-04T13:42:25.193107Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"model_name = 'NousResearch/Llama-2-7b-chat-hf'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:26.631142Z","iopub.execute_input":"2025-06-04T13:42:26.631378Z","iopub.status.idle":"2025-06-04T13:42:26.634825Z","shell.execute_reply.started":"2025-06-04T13:42:26.631362Z","shell.execute_reply":"2025-06-04T13:42:26.634120Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"dataset_name = 'chrishayuk/test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:28.382869Z","iopub.execute_input":"2025-06-04T13:42:28.383142Z","iopub.status.idle":"2025-06-04T13:42:28.386672Z","shell.execute_reply.started":"2025-06-04T13:42:28.383121Z","shell.execute_reply":"2025-06-04T13:42:28.386078Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"new_model = 'llama-2-7b-chuk-test'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:31.831988Z","iopub.execute_input":"2025-06-04T13:42:31.832734Z","iopub.status.idle":"2025-06-04T13:42:31.835976Z","shell.execute_reply.started":"2025-06-04T13:42:31.832711Z","shell.execute_reply":"2025-06-04T13:42:31.835124Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"output_dir = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:32.609226Z","iopub.execute_input":"2025-06-04T13:42:32.609657Z","iopub.status.idle":"2025-06-04T13:42:32.613169Z","shell.execute_reply.started":"2025-06-04T13:42:32.609636Z","shell.execute_reply":"2025-06-04T13:42:32.612451Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"num_train_epochs = 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:33.376443Z","iopub.execute_input":"2025-06-04T13:42:33.377074Z","iopub.status.idle":"2025-06-04T13:42:33.380252Z","shell.execute_reply.started":"2025-06-04T13:42:33.377052Z","shell.execute_reply":"2025-06-04T13:42:33.379531Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type = 'nf4',\n    bnb_4bit_compute_dtype = torch.float16,\n    bnb_4bit_use_double_quant = False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:33.997489Z","iopub.execute_input":"2025-06-04T13:42:33.998089Z","iopub.status.idle":"2025-06-04T13:42:34.003127Z","shell.execute_reply.started":"2025-06-04T13:42:33.998067Z","shell.execute_reply":"2025-06-04T13:42:34.002520Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:37.630252Z","iopub.execute_input":"2025-06-04T13:42:37.630992Z","iopub.status.idle":"2025-06-04T13:42:37.789953Z","shell.execute_reply.started":"2025-06-04T13:42:37.630965Z","shell.execute_reply":"2025-06-04T13:42:37.789433Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    model_name, \n    quantization_config = bnb_config,\n    device_map = {\"\": 0}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:42:38.648381Z","iopub.execute_input":"2025-06-04T13:42:38.649087Z","iopub.status.idle":"2025-06-04T13:43:12.025456Z","shell.execute_reply.started":"2025-06-04T13:42:38.649062Z","shell.execute_reply":"2025-06-04T13:43:12.024878Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c495f267f0df4c0899aa4108de07ba6e"}},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:16.430651Z","iopub.execute_input":"2025-06-04T13:43:16.430940Z","iopub.status.idle":"2025-06-04T13:43:16.434714Z","shell.execute_reply.started":"2025-06-04T13:43:16.430919Z","shell.execute_reply":"2025-06-04T13:43:16.433987Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"model.config.pretraining_tp = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:18.341649Z","iopub.execute_input":"2025-06-04T13:43:18.341952Z","iopub.status.idle":"2025-06-04T13:43:18.345727Z","shell.execute_reply.started":"2025-06-04T13:43:18.341929Z","shell.execute_reply":"2025-06-04T13:43:18.344956Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True, use_fast = False)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:21.830155Z","iopub.execute_input":"2025-06-04T13:43:21.830445Z","iopub.status.idle":"2025-06-04T13:43:22.364038Z","shell.execute_reply.started":"2025-06-04T13:43:21.830419Z","shell.execute_reply":"2025-06-04T13:43:22.363376Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"prompt = 'Write a hello world program in the OPL programming language.'\npipe = pipeline(task = 'text-generation', model = model, tokenizer = tokenizer, max_length = 200)\nresult = pipe(f'<s>[INST] {prompt} [/INST]')\nprint(result[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:26.299061Z","iopub.execute_input":"2025-06-04T13:43:26.299661Z","iopub.status.idle":"2025-06-04T13:43:35.034950Z","shell.execute_reply.started":"2025-06-04T13:43:26.299638Z","shell.execute_reply":"2025-06-04T13:43:35.034297Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] Write a hello world program in the OPL programming language. [/INST]  Sure! Here is an example of a \"Hello World\" program in the OPL programming language:\n```\nPRINT \"Hello, World!\"\n```\nThis program will output the string \"Hello, World!\" to the console when run.\n\nHere's a breakdown of the syntax:\n\n* `PRINT`: This keyword is used to print a value to the console.\n* `\":\"`: This is the colon used to separate the value from the rest of the program.\n* `\"Hello, World!\"`: This is the value that will be printed to the console.\n\nThat's it! This is the basic syntax of an OPL program. Of course, there are many more features and capabilities of the OPL language that you can explore as you learn more about it.\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:41.236699Z","iopub.execute_input":"2025-06-04T13:43:41.237286Z","iopub.status.idle":"2025-06-04T13:43:41.245213Z","shell.execute_reply.started":"2025-06-04T13:43:41.237265Z","shell.execute_reply":"2025-06-04T13:43:41.244617Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:43.051065Z","iopub.execute_input":"2025-06-04T13:43:43.051660Z","iopub.status.idle":"2025-06-04T13:43:44.289174Z","shell.execute_reply.started":"2025-06-04T13:43:43.051637Z","shell.execute_reply":"2025-06-04T13:43:44.288626Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:49.387045Z","iopub.execute_input":"2025-06-04T13:43:49.387663Z","iopub.status.idle":"2025-06-04T13:43:49.392628Z","shell.execute_reply.started":"2025-06-04T13:43:49.387637Z","shell.execute_reply":"2025-06-04T13:43:49.391978Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"{'text': '<s>[INST] What does OPL stand for in the OPL programming language? [/INST] OPL is short for Open Programming Language </s>'}"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(\n        examples['text'],\n        truncation=True,\n        padding='max_length',\n        max_length=512\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:52.390254Z","iopub.execute_input":"2025-06-04T13:43:52.390968Z","iopub.status.idle":"2025-06-04T13:43:52.394737Z","shell.execute_reply.started":"2025-06-04T13:43:52.390945Z","shell.execute_reply":"2025-06-04T13:43:52.393940Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:43:55.758618Z","iopub.execute_input":"2025-06-04T13:43:55.758890Z","iopub.status.idle":"2025-06-04T13:43:55.770856Z","shell.execute_reply.started":"2025-06-04T13:43:55.758871Z","shell.execute_reply":"2025-06-04T13:43:55.770134Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha = 16,\n    r = 64,\n    bias = 'none',\n    task_type = 'CAUSAL_LM'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:44:07.948205Z","iopub.execute_input":"2025-06-04T13:44:07.948691Z","iopub.status.idle":"2025-06-04T13:44:07.952210Z","shell.execute_reply.started":"2025-06-04T13:44:07.948667Z","shell.execute_reply":"2025-06-04T13:44:07.951459Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir = output_dir,\n    num_train_epochs = num_train_epochs,\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 8,\n    optim = 'paged_adamw_8bit',\n    save_steps = 0,\n    logging_steps = 10,\n    learning_rate = 2e-4,\n    weight_decay = 0.001,\n    fp16 = True,\n    bf16 = False,\n    max_grad_norm = 0.3,\n    max_steps = -1,\n    warmup_ratio = 0.03,\n    group_by_length = True,\n    lr_scheduler_type = 'cosine',\n    report_to = 'tensorboard'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:44:11.372421Z","iopub.execute_input":"2025-06-04T13:44:11.373147Z","iopub.status.idle":"2025-06-04T13:44:11.405369Z","shell.execute_reply.started":"2025-06-04T13:44:11.373121Z","shell.execute_reply":"2025-06-04T13:44:11.404859Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    train_dataset = tokenized_dataset,\n    peft_config = peft_config,\n    args = training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:44:14.797280Z","iopub.execute_input":"2025-06-04T13:44:14.797870Z","iopub.status.idle":"2025-06-04T13:44:15.552091Z","shell.execute_reply.started":"2025-06-04T13:44:14.797847Z","shell.execute_reply":"2025-06-04T13:44:15.551506Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T13:44:22.231856Z","iopub.execute_input":"2025-06-04T13:44:22.232358Z","iopub.status.idle":"2025-06-04T13:44:22.235779Z","shell.execute_reply.started":"2025-06-04T13:44:22.232335Z","shell.execute_reply":"2025-06-04T13:44:22.235017Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"print(\"Starting training...\")\ntrainer.train()\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:22:29.954205Z","iopub.execute_input":"2025-06-04T14:22:29.954499Z","iopub.status.idle":"2025-06-04T14:33:03.003803Z","shell.execute_reply.started":"2025-06-04T14:22:29.954475Z","shell.execute_reply":"2025-06-04T14:33:03.002983Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 10:18, Epoch 19/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.105500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.079600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.052000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.040100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training complete!\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:33:11.004355Z","iopub.execute_input":"2025-06-04T14:33:11.005088Z","iopub.status.idle":"2025-06-04T14:33:11.423692Z","shell.execute_reply.started":"2025-06-04T14:33:11.005065Z","shell.execute_reply":"2025-06-04T14:33:11.423038Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"('llama-2-7b-chuk-test/tokenizer_config.json',\n 'llama-2-7b-chuk-test/special_tokens_map.json',\n 'llama-2-7b-chuk-test/tokenizer.model',\n 'llama-2-7b-chuk-test/added_tokens.json')"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"print(f\"Model and tokenizer saved to {new_model}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:33:16.780679Z","iopub.execute_input":"2025-06-04T14:33:16.781340Z","iopub.status.idle":"2025-06-04T14:33:16.785721Z","shell.execute_reply.started":"2025-06-04T14:33:16.781313Z","shell.execute_reply":"2025-06-04T14:33:16.784929Z"}},"outputs":[{"name":"stdout","text":"Model and tokenizer saved to llama-2-7b-chuk-test\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"prompt = 'Write a tax calculator in OPL'\npipe = pipeline(task = 'text-generation', model = model, tokenizer = tokenizer, max_length = 200)\nresult = pipe(f'<s>[INST] {prompt} [/INST]')\nprint(result[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:37:52.467823Z","iopub.execute_input":"2025-06-04T14:37:52.468415Z","iopub.status.idle":"2025-06-04T14:38:00.122685Z","shell.execute_reply.started":"2025-06-04T14:37:52.468382Z","shell.execute_reply":"2025-06-04T14:38:00.121911Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] Write a tax calculator in OPL [/INST] PROC main:\n\tLOCAL percentage%\n\tLOCAL income%\n\tLOCAL tax%\n\tPRINT \"Tax Calculator\\n\"\n\tPRINT \"Enter your income:\"\n\tINPUT income%\n\tPRINT \"Enter the tax rate (%):\"\n\tINPUT percentage%\n\ttax% = (income% * percentage%) / 100.0\n\tPRINT \"Tax:\", tax%\n\tGET\nENDP \n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"prompt = 'Write a Hello Chris program in psion opl'\npipe = pipeline(task = 'text-generation', model = model, tokenizer = tokenizer, max_length = 200)\nresult = pipe(f'<s>[INST] {prompt} [/INST]')\nprint(result[0]['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T14:40:57.805949Z","iopub.execute_input":"2025-06-04T14:40:57.806218Z","iopub.status.idle":"2025-06-04T14:41:00.154251Z","shell.execute_reply.started":"2025-06-04T14:40:57.806198Z","shell.execute_reply":"2025-06-04T14:41:00.153639Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] Write a Hello Chris program in psion opl [/INST] PROC Hello:\n\tLOCAL name$(10)\n\tPRINT \"Hello \", name$\n\tGET\nENDP \n","output_type":"stream"}],"execution_count":75}]}